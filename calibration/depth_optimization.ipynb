{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import sys, cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('C:/Users/owner/Documents/GitHub/Scalable-Hyperspectral-3D-Imaging')\n",
    "from hyper_sl.utils import _constants as C\n",
    "\n",
    "device = 'cuda:0'\n",
    "binning = 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid pattern 생성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input pixel location where white dots exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  D/fine the size of the image\n",
    "WIDTH = 1280 // binning\n",
    "HEIGHT = 720 // binning\n",
    "\n",
    "# Define the size of the dot and its center coordinate\n",
    "DOT_SIZE = 5\n",
    "# CENTER_COORDS = [(100, 100), (100, 200), (100, 300), (300, 100), (300, 200), (300, 300), (500, 100), (500, 200), (500, 300)]\n",
    "CENTER_COORDS = [ (480, 180)]\n",
    "\n",
    "# # Create a tensor to store the dots\n",
    "dots = torch.zeros((HEIGHT, WIDTH))\n",
    "\n",
    "# Loop through each center coordinate and set the pixels of the corresponding dot to 1\n",
    "\n",
    "for center_x, center_y in CENTER_COORDS:\n",
    "    x_start = center_x - DOT_SIZE // 2\n",
    "    y_start = center_y - DOT_SIZE // 2\n",
    "    x_end = x_start + DOT_SIZE\n",
    "    y_end = y_start + DOT_SIZE\n",
    "    dots[ y_start:y_end, x_start:x_end] = 1\n",
    "    \n",
    "# Convert the tensor to a PIL image\n",
    "grid = dots\n",
    "\n",
    "# Display the imageplt.imshow(grid)\n",
    "# plt.imsave('./grid.png', grid.numpy())\n",
    "cv2.imwrite('./dot_480_180.png', grid.numpy()*255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsic of camera\n",
    "cam_H, cam_W = 1536//binning, 2048//binning\n",
    "# sensor_width_cam = 5.32 * 1e-3\n",
    "cam_pitch = 3.45*1e-6*2\n",
    "focal_length_cam = (1.75261231e+03)*cam_pitch # 12 mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsic of projector\n",
    "proj_sensor_diag = C.PROJ_DIAG *1e-3\n",
    "proj_H = 720 // binning\n",
    "proj_W = 1280 // binning\n",
    "sensor_width_proj = torch.sin(torch.atan2(torch.tensor(proj_H),torch.tensor(proj_H)))*proj_sensor_diag\n",
    "proj_pitch = 5.73 *1e-6 * 2\n",
    "# proj_pitch = (sensor_width_proj/ proj_H)\n",
    "proj_focal_length = (1.04829625e+03)*proj_pitch\n",
    "# proj_focal_length = 14*1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_int = torch.tensor([[1.75261231e+03, 0.00000000e+00 ,4.99891309e+02],\n",
    " [0.00000000e+00 ,1.76071560e+03 , 4.15817088e+02],\n",
    " [0.00000000e+00 ,0.00000000e+00, 1.00000000e+00]], device= device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_int = torch.tensor([[1.04829625e+03 , 0.00000000e+00 ,2.76519034e+02],\n",
    " [0.00000000e+00 ,1.04580837e+03  ,3.10408841e+02],\n",
    " [0.00000000e+00 ,0.00000000e+00, 1.00000000e+00]] , device= device)\n",
    "\n",
    "proj_dist = torch.tensor([[-1.21868710e-02 , 5.29790580e-01, -2.81219998e-03, -1.95464376e-02,-5.36525586e+00]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_grid = cv2.undistort(grid.numpy(), proj_int.detach().cpu().numpy(), proj_dist.numpy())\n",
    "dst_grid = torch.tensor(dst_grid).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth 알아내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 1024, 6.9e-06, 0.012093024939)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_H, cam_W, cam_pitch, focal_length_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 640, 1.146e-05, 0.012013475025000001, 0.005842, tensor(0.0041))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_H, proj_W, proj_pitch, proj_focal_length, proj_sensor_diag, sensor_width_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.5393e-04,  0.0000e+00, -2.6378e-01],\n",
       "        [ 0.0000e+00,  9.5620e-04, -2.9681e-01],\n",
       "        [ 0.0000e+00,  0.0000e+00,  1.0000e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.inv(proj_int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unprojection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unprojection(depth, r, c):\n",
    "        \"\"\" Unproject camera sensor coord plane to world coord\n",
    "\n",
    "            input : depth\n",
    "            return : world coordinate X,Y,Z\n",
    "            \n",
    "        \"\"\"\n",
    "        # c, r = torch.meshgrid(torch.linspace(0,proj_H-1,proj_H), torch.linspace(0,proj_W-1,proj_W), indexing='ij') # 행렬 indexing        \n",
    "        # c, r = c.reshape(proj_W*proj_H), r.reshape(proj_W*proj_H)\n",
    "        ones = torch.ones_like(r, device=device)\n",
    "        \n",
    "        xyz_p = torch.stack((r*depth,c*depth,ones*depth), dim = 0)\n",
    "        \n",
    "        XYZ = torch.linalg.inv(proj_int)@xyz_p\n",
    "        \n",
    "        return XYZ, r, c"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### extrinsic matrix of projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrinsic_proj_real():\n",
    "        \"\"\" World coordinate to real proj's coordinate\n",
    "        \n",
    "        \"\"\"\n",
    "        extrinsic_proj_real = torch.zeros((4,4), device= device)\n",
    "        \n",
    "        \n",
    "        extrinsic_proj_real[:3,:3] = torch.tensor([[ 0.99401968,  0.01234548 , 0.10850096],\n",
    "                                                        [-0.00400605,  0.99704272, -0.0767448 ],\n",
    "                                                        [-0.10912754, 0.07585118  ,0.99112955]])\n",
    "\n",
    "        t_mtrx = torch.tensor([[-65.40436873],\n",
    "                                [-14.71175827],\n",
    "                                [24.13928588]])\n",
    "        \n",
    "        extrinsic_proj_real[:3,3:4] = t_mtrx*1e-3\n",
    "        extrinsic_proj_real[3,3] = 1\n",
    "        \n",
    "        extrinsic_proj_real = torch.linalg.inv(extrinsic_proj_real)[:3]\n",
    "        return extrinsic_proj_real"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection(XYZ):\n",
    "        \"\"\"\n",
    "            proj coord to world coord\n",
    "        \"\"\"\n",
    "        ones = torch.ones(size=(1,XYZ.shape[1]), device= device)\n",
    "        \n",
    "        XYZ1 = torch.concat((XYZ, ones), dim = 0)\n",
    "\n",
    "        # XYZ coords in world coordinate               \n",
    "        uv_cam = (cam_int)@(extrinsic_proj_real())@XYZ1.to(device= device)\n",
    "        uv_cam = uv_cam / uv_cam[2]\n",
    "\n",
    "        return uv_cam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### find the idx of white camera pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_idx(xy_cam, illum, r, c):\n",
    "    \n",
    "    r_cam, c_cam = xy_cam[1], xy_cam[0]\n",
    "\n",
    "    cond = (0<= r_cam)*(r_cam < cam_H)*(0<=c_cam)*(c_cam< cam_W)\n",
    "\n",
    "    r_cam_valid, c_cam_valid = r_cam[cond], c_cam[cond]\n",
    "    r_cam_valid, c_cam_valid = torch.tensor(r_cam_valid), torch.tensor(c_cam_valid)\n",
    "\n",
    "    new_idx = cam_W * r_cam_valid.long() + c_cam_valid.long()      \n",
    "\n",
    "    # captured_img = torch.zeros(size=(cam_H*cam_W,))\n",
    "\n",
    "    # illum = illum.reshape(proj_H*proj_W)\n",
    "    \n",
    "    # proj_r, proj_c = r[cond], c[cond]\n",
    "    # new_idx_illum = proj_W * proj_c.long() + proj_r.long()\n",
    "    \n",
    "    # captured_img[new_idx] = illum.reshape(proj_H* proj_W)[new_idx_illum]\n",
    "    \n",
    "    # captured_img = captured_img.reshape(cam_H, cam_W)\n",
    "    \n",
    "    # return captured_img, new_idx\n",
    "    \n",
    "    return new_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth = torch.zeros(size=(proj_H* proj_W,))\n",
    "\n",
    "# depth[:] = 1.7\n",
    "\n",
    "# r = torch.tensor([100, 300, 500, 100, 300, 500, 100, 300, 500], device=device)\n",
    "# c = torch.tensor([100, 200, 300, 100, 200, 300, 100, 200, 300], device=device)\n",
    "\n",
    "# # Unproject to 3D points\n",
    "# XYZ,r,c = unprojection(depth=depth,r,c)\n",
    "\n",
    "# # Projector to projector\n",
    "# uv_cam = projection(XYZ)\n",
    "\n",
    "# # new_idx = find_idx(xy_cam, torch.tensor(dst_grid), r, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cam_int = torch.tensor([[1.75261231e+03, 0.00000000e+00 ,4.99891309e+02],\n",
    "#  [0.00000000e+00 ,1.76071560e+03, 4.15817088e+02],\n",
    "#  [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])\n",
    "\n",
    "cam_dist = torch.tensor([[-2.44349013e-01, 6.10520025e-01, -7.59154684e-04 ,-1.58477294e-03, -5.52344502e+00]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dst = cv2.undistort(captured_img.numpy(), cam_int.numpy(), cam_dist.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_idx_real():  \n",
    "    real_x = torch.tensor([89, 428, 763, 82, 424, 762, 76, 423, 764], device = 'cuda:0') # 윗줄부터 아랫줄 찬찬히\n",
    "    real_y = torch.tensor([207, 210, 216, 374, 379, 282, 546, 522, 552], device = 'cuda:0')\n",
    "    ones = torch.ones_like(real_x, device=device)\n",
    "    \n",
    "    real_xy1 = torch.stack((real_x, real_y, ones), dim = 0)\n",
    "    \n",
    "    return real_xy1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth value to be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized depth value :  tensor([0.4082], device='cuda:0', requires_grad=True)\n",
      " Depth value : tensor([0.4182], device='cuda:0', requires_grad=True), Epoch : 0/10000, Loss: 131.22557067871094, LR: 0.01\n",
      " Depth value : tensor([1.4741], device='cuda:0', requires_grad=True), Epoch : 1000/10000, Loss: 54.57651901245117, LR: 0.005\n",
      " Depth value : tensor([1.4740], device='cuda:0', requires_grad=True), Epoch : 2000/10000, Loss: 54.57665252685547, LR: 0.0025\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25440\\1013322007.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# Projector to projector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0muv_cam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprojection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXYZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# find camera index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25440\\883495475.py\u001b[0m in \u001b[0;36mprojection\u001b[1;34m(XYZ)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# XYZ coords in world coordinate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0muv_cam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcam_int\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m@\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextrinsic_proj_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mXYZ1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0muv_cam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muv_cam\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0muv_cam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25440\\3243409204.py\u001b[0m in \u001b[0;36mextrinsic_proj_real\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         extrinsic_proj_real[:3,:3] = torch.tensor([[ 0.99401968,  0.01234548 , 0.10850096],\n\u001b[0;32m      9\u001b[0m                                                         \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.00400605\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0.99704272\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.0767448\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                                                         [-0.10912754, 0.07585118  ,0.99112955]])\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         t_mtrx = torch.tensor([[-65.40436873],\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimize depth value\n",
    "\n",
    "r = torch.tensor([100, 300, 500, 100, 300, 500, 100, 300, 500], device=device)\n",
    "c = torch.tensor([100, 200, 300, 100, 200, 300, 100, 200, 300], device=device)\n",
    "\n",
    "epoch = 10000\n",
    "loss_f = torch.nn.L1Loss()\n",
    "\n",
    "lr = 1e-2\n",
    "decay_step = 1000\n",
    "\n",
    "losses = []\n",
    "\n",
    "depth_value = torch.rand(1, requires_grad = True, device = device)\n",
    "\n",
    "print('initialized depth value : ', depth_value)\n",
    "\n",
    "optimizer = torch.optim.Adam([depth_value], lr = lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=decay_step, gamma = 0.5)\n",
    "\n",
    "for i in range(epoch):\n",
    "    # depth value\n",
    "    depth = torch.zeros(size=(r.shape[0],), device=device)\n",
    "    depth[:] = depth_value\n",
    "    # depth[:] = torch.tensor([1.7], requires_grad=True, device= device)\n",
    "\n",
    "    # Unproject to 3D points\n",
    "    XYZ, r,c = unprojection(depth=depth, r=r, c=c)\n",
    "\n",
    "    # Projector to projector\n",
    "    uv_cam = projection(XYZ)\n",
    "\n",
    "    # find camera index\n",
    "    # new_idx = find_idx(xy_cam, torch.tensor(dst_grid), r, c)\n",
    "    \n",
    "    # index of real captured img\n",
    "    real_xy1 = find_idx_real()\n",
    "    \n",
    "    loss = loss_f(uv_cam,real_xy1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    losses.append(loss.item())\n",
    "    optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "            # print(depth)\n",
    "            print(f\" Depth value : {depth_value}, Epoch : {i}/{epoch}, Loss: {loss.item()}, LR: {optimizer.param_groups[0]['lr']}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffraction grating calibration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### camera pixel position from catured image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 파장의 빛이 있는 camera pixel 위치 알아내기?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### camera pixel position from unproj & proj method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyper3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fa623eb3390c47c076f568bf88f24d6007a788d3fe47958c8bfce56c724a2ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
